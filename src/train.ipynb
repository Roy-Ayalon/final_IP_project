{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855e873d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dfd96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import tqdm\n",
    "from dataset import WheatSegDataset\n",
    "from unet import UNet\n",
    "from definitions import *\n",
    "\n",
    "# Select MPS if available, otherwise CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4fae4d",
   "metadata": {},
   "source": [
    "# DataLoaders, Model, Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04278e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2699 matching image-mask pairs in /Users/royayalon/Documents/Academy/final_IP_project/data/train\n",
      "Found 674 matching image-mask pairs in /Users/royayalon/Documents/Academy/final_IP_project/data/val\n",
      "number of training samples: 2699\n",
      "number of validation samples: 674\n",
      "dataloaders created with batch size 4 and 1 workers\n",
      "=== Dataloaders Summary ===\n",
      "Train Loader: 675 batches\n",
      "Validation Loader: 169 batches\n",
      "=== Model Summary ===\n",
      "UNet(\n",
      "  (downs): ModuleList(\n",
      "    (0): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (1): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (3): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): DoubleConv(\n",
      "    (net): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (ups): ModuleList(\n",
      "    (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (4): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (5): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (7): DoubleConv(\n",
      "      (net): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = WheatSegDataset(\n",
    "    images_dir=\"/Users/royayalon/Documents/Academy/final_IP_project/data/train\",\n",
    "    masks_dir=\"/Users/royayalon/Documents/Academy/final_IP_project/data/train_masks\"\n",
    ")\n",
    "\n",
    "val_dataset = WheatSegDataset(\n",
    "    images_dir=\"/Users/royayalon/Documents/Academy/final_IP_project/data/val\",\n",
    "    masks_dir=\"/Users/royayalon/Documents/Academy/final_IP_project/data/val_masks\"\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False)\n",
    "\n",
    "print(f\"number of training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"number of validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"dataloaders created with batch size {BATCH_SIZE} and {NUM_WORKERS} workers\")\n",
    "print(f\"=== Dataloaders Summary ===\")\n",
    "print(f\"Train Loader: {len(train_loader)} batches\")\n",
    "print(f\"Validation Loader: {len(val_loader)} batches\")\n",
    "\n",
    "\n",
    "model   = UNet().to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"=== Model Summary ===\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486c026f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 2699\n",
      "Val dataset length: 674\n",
      "First item shapes - Image: torch.Size([3, 1024, 1024]), Mask: torch.Size([1, 1024, 1024])\n",
      "Dataset access successful!\n"
     ]
    }
   ],
   "source": [
    "# Debug: Test dataset creation and access\n",
    "print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "print(f\"Val dataset length: {len(val_dataset)}\")\n",
    "\n",
    "# Try to access the first item\n",
    "try:\n",
    "    first_item = train_dataset[0]\n",
    "    print(f\"First item shapes - Image: {first_item[0].shape}, Mask: {first_item[1].shape}\")\n",
    "    print(\"Dataset access successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing first item: {e}\")\n",
    "    \n",
    "# Check if datasets have any items\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"ERROR: Train dataset is empty!\")\n",
    "if len(val_dataset) == 0:\n",
    "    print(\"ERROR: Validation dataset is empty!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b815c",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69535046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/675 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "results = {\"train_loss\": [], \"validation_loss\": []}\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, masks in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    results[\"train_loss\"].append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    results[\"validation_loss\"].append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02894126",
   "metadata": {},
   "source": [
    "# Save Model & Plot Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bca41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "torch.save(model.state_dict(), \"unet_baseline_mps.pth\")\n",
    "\n",
    "# (Optional) plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, NUM_EPOCHS + 1)\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, results[\"train_loss\"], '-o')\n",
    "plt.title(\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, results[\"validation_loss\"], '-o')\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wheat-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
